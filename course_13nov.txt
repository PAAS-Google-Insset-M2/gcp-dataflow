November 13th

BigQuery StudioExplorator ?

With one table ?
Use of data pipelines with Apache Beam

Create a pipeline => from bucket data to BigQuery table

Beam -> dataflow runner (main.py) & directrunner (main.py, bigquery.py, )
4 possible runners => 
apache flink runner, 
apache spark console (a lot of data !), 
(directrunner) => slowest (local execution),
DataflowRunner => executed on gcp

Beam => treatment/package ?

Using ParDo => each p collection item will be transmitted to the function for treatment !

- Reading the list of items
- Reading a record of bucket data & retrieving the list of data

env with gcp app credentials, bucket_name, project_id & job_name

